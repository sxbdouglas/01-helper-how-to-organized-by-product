https://www.projectpro.io/tutorial



Optimization (Best Practices) ========================================================================
    https://surenderpa.medium.com/apache-spark-optimization-techniques-b7ced0b9c218

#Checklist
    
    1 - filter rows and columns as early as possible, and clean up null columns to avoid issues with further operations
                       (Spark doesn’t support column === null)
        examples:

          Null filter

            df.filter("state is NULL").show(false)
            df.filter(df("state").isNull).show(false)
            df.filter(col("state").isNull).show(false) //Required col function import
            df.filter("state is not NULL").show(false)
            df.filter("NOT state is NULL").show(false)
            df.filter(df("state").isNotNull).show(false)
            df.filter(col("state").isNotNull).show(false) //Required col function import

            df.createOrReplaceTempView("DATA")
            spark.sql("SELECT * FROM DATA where STATE IS NULL").show(false)
            spark.sql("SELECT * FROM DATA where STATE IS NULL AND GENDER IS NULL").show(false)
            spark.sql("SELECT * FROM DATA where STATE IS NOT NULL").show(false)

          DataFrame filter() with Column condition (works better with data frames)
            
            df.filter(df("state") === "OH").show(false)
            df.filter('state === "OH").show(false)
            df.filter($state === "OH").show(false)
            df.filter(col("state") === "OH").show(false)
            df.where(df("state") === "OH").show(false)
            df.where('state === "OH").show(false)
            df.where($state === "OH").show(false)
            df.where(col("state") === "OH").show(false)

            #pyspark (only?)
            df.filter(df.colD.between(200, 400))
            df.filter(df.colC >= 3.0)


          DataFrame filter() with SQL Expression
            
            df.filter("gender == 'M'").show(false)
            df.where("gender == 'M'").show(false)

          Filter with Multiple Conditions (and(&&), or(||), not(!))

            df.filter(df("state") === "OH" && df("gender") === "M").show(false)

          Filter on an Array Column

            import org.apache.spark.sql.functions.array_contains
            df.filter(array_contains(df("languages"),"Java")).show(false)
          
          Filter on Nested Struct columns

            df.filter(df("name.lastname") === "Williams").show(false)

          Drop rows with nulls

            df.na.drop().show(false) #drops all rows doesn't matter the collumn with null
            df.na.drop("any").show(false) #same behavior as the previous command.
            df.na.drop("all").show(false) #same behavior as the previous command.

          Drop Rows with NULL Values on Selected Columns

            df.na.drop(Seq("population","type")).show(false)


          Treating null Values

            #Replace all integer and long columns
              df.na.fill(0).show(false)

            #Replace with specific columns
              df.na.fill(0,Array("population")).show(false)

            #Replace all string columns
              df.na.fill("").show(false)

            #Replace with specific columns with "" by unknown
              df.na.fill("unknown",Array("city")).na.fill("",Array("type")).show(false)

    2 - File Format Selection

          AVRO - Row based file format (Write: Faster, Read: Slower)
              Useful on landing zones, ingestions from sources or streamings 
          Parquet, ORC - Columnar based file format (Write: Slower, Read: Faster)
              Useful for frequently reading and transformations
              Parquet has Data skipping what reads only necessary rows and columns.

    3 - Caching

          There are two functions available in Spark Cache and Persist. 
          Cache saves your data in memory and if spilled the data is written to disk. 
          Persist function gives you an option to specify where to cache your data and we can cache in memory or disk. 
          Be careful with the amount of data being cached in memory because cached data would occupy your compute memory and you may want to make sure you have sufficient compute memory to process your data. 
          Persisting the cache on disk can be considered since it would not occupy your compute power. 
          It is also a good practice to unpersist the cached dataframe using the unpersist() method once the computations are completed. 
          Do not cache a complete table into memory from storage because catalyst optimizer cannot apply query optimizations like predicate pushdown.

    4 - Enable Adaptive Query Execution (available in spark 3.0, default in spark 3.2)

          It changes the query plan at run time
          Turn ON using spark.conf.set (“spark.sql.adaptive.enabled”, True)
          When working out the query execution plan at query run time, 
             it can switch join strategies to choose between shuffle sort join or broadcast join based on the size of the tables at run time, 
             coalesce the number of shuffle partitions or optimize the skew joins

          Advantages: Join Strategy Switch, Coalesce Shuffle Partition, Optimize Skew joins


    5 - Broadcast join (when AQE is ON, spark can choose the optimal join threshold)

          Use broadcast join always it is possible.
          To set Broadcast table size use (less then 15MB is optimal):
            Spark.conf.set(“spark.sql.autoBroadcastJoinThreshold”, 10MB)

    6 - Optimize shuffle partition 

          Default is 200, but recommended is the number of cores in the cluster.
          For large files and small number of partition can cause Out Of Memory issues.
          Shuffle partition is not the same concept of Parquet Files Partitioning strategy. Think smart to avoid to many small files in your file system.


    7 - Avoid Skew(unevenly data distribution) and Spill(to disk, because not enough executor memory, since data is unevenly)

          





              

Avoid Sort merge join, it causes data shuffling between nodes which is expensive
Set spark.sql.shuffle.partitions proportional total number of cores in the cluster, with AQN spark will scale down based on run time statistics.
Data Skew occurs when the data is unevenly distributed among the data partitions in memory.

Optimization (Best Practices) ========================================================================


Data Frame ===========================================================================================
    https://www.projectpro.io/recipes/what-are-different-ways-create-dataframe-from-raw-data-spark
#From scratch

    import spark.implicits._

    val columns = Seq("language", "users_count")
    val data = Seq(("Java", "20000"), ("Python", "100000"), ("Scala", "3000"))

    val rdd = spark.sparkContext.parallelize(data)

    val dfFromRDD1 = rdd.toDF()
    dfFromRDD1.printSchema()

    val dfFromRDD2 = rdd.toDF("language", "users_count")
    dfFromRDD2.printSchema()

    val dfFromRDD3 = spark.createDataFrame(rdd).toDF(columns:_*)
    dfFromRDD3.printSchema()

#From CSV

    val df2 = spark.read.option("header", true).csv("/apps/FirstSparkProject/src/resources/Ages.csv")
    df2.show()
    
    df2.write.option("header", true).mode(SaveMode.Overwrite).csv("/apps/Ages2")


    val filePath="src/main/resources/small_zipcode.csv"
    val df=spark.read.options(Map("inferSchema"->"true","delimiter"->",","header"->"true")).csv(filePath)
    df.printSchema()
    df.show(false)


#From JSON

    val df = spark.read.json("/apps/zipcodes.json")
    df.printSchema()
    df.show(false)

    val multiline_df = spark.read.option("multiline", "true")
      .json("/apps/multiline-zipcode.json")
    multiline_df.show(false)

    df.write
      .json("/apps/zipcodes2")

#To Table
    df.createOrReplaceTempView("zips")
    val dfsql = spark.sql("SELECT * from zips")
    dfsql.printSchema()
    dfsql.show()

#With StructType

val arrayStructureData = Seq(
    Row(Row("James","","Smith"),List("Java","Scala","C++"),"OH","M"),
    Row(Row("Anna","Rose",""),List("Spark","Java","C++"),"NY","F"),
    Row(Row("Julia","","Williams"),List("CSharp","VB"),"OH","F"),
    Row(Row("Maria","Anne","Jones"),List("CSharp","VB"),"NY","M"),
    Row(Row("Jen","Mary","Brown"),List("CSharp","VB"),"NY","M"),
    Row(Row("Mike","Mary","Williams"),List("Python","VB"),"OH","M")
)

val arrayStructureSchema = new StructType()
    .add("name",new StructType()
    .add("firstname",StringType)
    .add("middlename",StringType)
    .add("lastname",StringType))
    .add("languages", ArrayType(StringType))
    .add("state", StringType)
    .add("gender", StringType)

val df = spark.createDataFrame(
    spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)
df.printSchema()
df.show()


root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- languages: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- state: string (nullable = true)
 |-- gender: string (nullable = true)

+--------------------+------------------+-----+------+
|                name|         languages|state|gender|
+--------------------+------------------+-----+------+
|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|
|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|
| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|
|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|
|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|
|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|
+--------------------+------------------+-----+------+


Data Frame ===========================================================================================


